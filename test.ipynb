{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cc23fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "import random\n",
    "import wandb\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f95e5700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "import scipy.interpolate as interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "466f5a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.9.1+cu128\n",
      "cuda available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print('cuda available:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6f04f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d13f952",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:"
     ]
    }
   ],
   "source": [
    "#load_dotenv()\n",
    "# get api key from env file\n",
    "#wandb.login(key=os.getenv('WANDB_API_KEY'))\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aa608a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Updated 'wandb=True'\n",
      "JSONDict(\"/home/smrutibiswal/.config/Ultralytics/settings.json\"):\n",
      "{\n",
      "  \"settings_version\": \"0.0.6\",\n",
      "  \"datasets_dir\": \"/home/smrutibiswal/Projects/datasets\",\n",
      "  \"weights_dir\": \"/home/smrutibiswal/Projects/edgeFleetAI/weights\",\n",
      "  \"runs_dir\": \"/home/smrutibiswal/Projects/edgeFleetAI/runs\",\n",
      "  \"uuid\": \"6ea89a9c93b11192ad287f89b850c3b86d278a64297084767728991e8f651f55\",\n",
      "  \"sync\": true,\n",
      "  \"api_key\": \"\",\n",
      "  \"openai_api_key\": \"\",\n",
      "  \"clearml\": true,\n",
      "  \"comet\": true,\n",
      "  \"dvc\": true,\n",
      "  \"hub\": true,\n",
      "  \"mlflow\": true,\n",
      "  \"neptune\": true,\n",
      "  \"raytune\": true,\n",
      "  \"tensorboard\": false,\n",
      "  \"wandb\": true,\n",
      "  \"vscode_msg\": true,\n",
      "  \"openvino_msg\": true\n",
      "}\n",
      "ðŸ’¡ Learn more about Ultralytics Settings at https://docs.ultralytics.com/quickstart/#ultralytics-settings\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enable yolo wandb logging using terminal command\n",
    "os.system('yolo settings wandb=True')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2bd1b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = os.path.join(os.getcwd(), 'cricket_ball_data')\n",
    "TEST_PATH = os.path.join(DATASET_PATH, 'test')\n",
    "TEST_IMAGES_PATH = os.path.join(TEST_PATH, 'images')\n",
    "TEST_LABELS_PATH = os.path.join(TEST_PATH, 'labels')\n",
    "TRAIN_PATH = os.path.join(DATASET_PATH, 'train')\n",
    "TRAIN_IMAGES_PATH = os.path.join(TRAIN_PATH, 'images')\n",
    "TRAIN_LABELS_PATH = os.path.join(TRAIN_PATH, 'labels')\n",
    "VALIDATION_PATH = os.path.join(DATASET_PATH, 'valid')\n",
    "VALIDATION_IMAGES_PATH = os.path.join(VALIDATION_PATH, 'images')\n",
    "VALIDATION_LABELS_PATH = os.path.join(VALIDATION_PATH, 'labels')\n",
    "MODEL_PATH = os.path.join(os.getcwd(), 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486f09ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a few training images with bounding boxes\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "for i in range(8):\n",
    "    image_files = os.listdir(TRAIN_IMAGES_PATH)\n",
    "    random_image_file = random.choice(image_files)\n",
    "    image_path = os.path.join(TRAIN_IMAGES_PATH, random_image_file)\n",
    "    if random_image_file.endswith('.jpg'):\n",
    "        label_file = random_image_file.replace('.jpg', '.txt')\n",
    "    elif random_image_file.endswith('.png'):\n",
    "        label_file = random_image_file.replace('.png', '.txt')\n",
    "    label_path = os.path.join(TRAIN_LABELS_PATH, label_file)\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    height, width, _ = image.shape\n",
    "\n",
    "    with open(label_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        class_id, x_center, y_center, bbox_width, bbox_height = map(float, line.strip().split())\n",
    "        x_center *= width\n",
    "        y_center *= height\n",
    "        bbox_width *= width\n",
    "        bbox_height *= height\n",
    "\n",
    "        x1 = int(x_center - bbox_width / 2)\n",
    "        y1 = int(y_center - bbox_height / 2)\n",
    "        x2 = int(x_center + bbox_width / 2)\n",
    "        y2 = int(y_center + bbox_height / 2)\n",
    "\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "\n",
    "    plt.subplot(2, 4, i + 1)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b13a26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the YOLOv8 model\n",
    "model = YOLO('yolov8s.pt')\n",
    "\n",
    "# train the model\n",
    "model.train(data=os.path.join(DATASET_PATH, 'dataset.yaml'),\n",
    "            epochs=20,\n",
    "            imgsz=640,\n",
    "            batch=8,\n",
    "            freeze=15,\n",
    "            device=DEVICE,\n",
    "            name='cricket_ball_detector')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a5f58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained model weights\n",
    "model.export(format='pt', weights=os.path.join(MODEL_PATH, 'cricket_ball_detector.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684c4105",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.export(format='onnx', weights=os.path.join(MODEL_PATH, 'cricket_ball_detector.onnx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe9a4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join(MODEL_PATH, 'cricket_ball_detector_model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0435ba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "results = model.val(data=os.path.join(DATASET_PATH, 'dataset.yaml'),\n",
    "                    imgsz=640,\n",
    "                    batch=8,\n",
    "                    device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5384cecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = YOLO(os.path.join(MODEL_PATH, 'cricket_ball_detector_model_v12.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ce0bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show detection results on a few test images\n",
    "plt.figure(figsize=(16, 8))\n",
    "for i in range(8):\n",
    "    image_files = os.listdir(TEST_IMAGES_PATH)\n",
    "    random_image_file = random.choice(image_files)\n",
    "    image_path = os.path.join(TEST_IMAGES_PATH, random_image_file)\n",
    "    results = model.predict(source=image_path,\n",
    "                            imgsz=800,\n",
    "                            conf=0.2,\n",
    "                            device=DEVICE)\n",
    "\n",
    "    result_image = results[0].plot()\n",
    "    result_image = cv2.cvtColor(result_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    plt.subplot(2, 4, i + 1)\n",
    "    plt.imshow(result_image)\n",
    "    plt.axis('off')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711ac129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# track a video using the trained model\n",
    "TEST_VIDS_PATH = os.path.join(os.getcwd(), 'test_vids')\n",
    "\n",
    "for vid_file in os.listdir(TEST_VIDS_PATH):\n",
    "    vid_path = os.path.join(TEST_VIDS_PATH, vid_file)\n",
    "    results = model.predict(source=vid_path,\n",
    "                            imgsz=2048,\n",
    "                            conf=0.2,\n",
    "                            device=DEVICE,\n",
    "                            save=True,\n",
    "                            project='model',\n",
    "                            name='inference_results',\n",
    "                            exist_ok=True)\n",
    "    print(f'Processed video saved at: {os.path.join(MODEL_PATH, \"inference_results\", vid_file)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e445e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# track the ball in the test_vids\n",
    "for vid_file in os.listdir(TEST_VIDS_PATH):\n",
    "    vid_path = os.path.join(TEST_VIDS_PATH, vid_file)\n",
    "    results = model.track(source=vid_path,\n",
    "                          persist=False,\n",
    "                          save=True,\n",
    "                          imgsz=2048,\n",
    "                          conf=0.4,\n",
    "                          project='model',\n",
    "                          name='tracking_results',\n",
    "                          exist_ok=True)\n",
    "    print(f'Tracking video saved at: {os.path.join(MODEL_PATH, \"tracking_results\", vid_file)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "id": "75c4c03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# HELPER: SCALED ROI SELECTION\n",
    "# ==========================================\n",
    "def select_roi_scaled(window_name, frame, target_width=1280):\n",
    "    \"\"\"\n",
    "    Opens a scaled-down version of the frame for ROI selection, \n",
    "    then maps the coordinates back to the original resolution.\n",
    "    \"\"\"\n",
    "    h, w = frame.shape[:2]\n",
    "    scale = target_width / w\n",
    "    \n",
    "    # Resize for display\n",
    "    small_frame = cv2.resize(frame, (target_width, int(h * scale)))\n",
    "    \n",
    "    # Select ROI on the small frame\n",
    "    # Returns (x, y, w, h)\n",
    "    roi_small = cv2.selectROI(window_name, small_frame, showCrosshair=True)\n",
    "    cv2.destroyWindow(window_name)\n",
    "    \n",
    "    # Map back to original coordinates\n",
    "    if roi_small[2] > 0 and roi_small[3] > 0: # If width and height > 0\n",
    "        x = int(roi_small[0] / scale)\n",
    "        y = int(roi_small[1] / scale)\n",
    "        w = int(roi_small[2] / scale)\n",
    "        h = int(roi_small[3] / scale)\n",
    "        return (x, y, w, h)\n",
    "    return (0, 0, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1194,
   "id": "2bf2bba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching Setup...\n",
      "Selected ROIs:\n",
      "Active Play Area: (402, 28, 1344, 898)\n",
      "Start Zone: (690, 46, 441, 282)\n",
      "\n",
      "Processing Video (Resolution: 1920x1080)...\n",
      "Frame 0 processed, detection: None\n",
      "Frame 1 processed, detection: None\n",
      "Frame 2 processed, detection: None\n",
      "Frame 3 processed, detection: None\n",
      "Frame 4 processed, detection: None\n",
      "LOCKED ON at Frame 5: (np.float32(898.2599), np.float32(168.03787))\n",
      "Frame 5 processed, detection: (np.float32(898.2599), np.float32(168.03787))\n",
      "Frame 6 processed, detection: (np.float32(906.7438), np.float32(166.49026))\n",
      "Frame 7 processed, detection: (np.float32(913.0237), np.float32(166.47955))\n",
      "Frame 8 processed, detection: (np.float32(918.3371), np.float32(172.3624))\n",
      "Frame 9 processed, detection: (np.float32(897.0719), np.float32(234.38097))\n",
      "Frame 10 processed, detection: (np.float32(885.33136), np.float32(228.20984))\n",
      "Frame 11 processed, detection: (np.float32(931.7075), np.float32(199.24944))\n",
      "Frame 12 processed, detection: (np.float32(902.5604), np.float32(273.55188))\n",
      "Frame 13 processed, detection: (np.float32(888.6969), np.float32(272.826))\n",
      "Frame 14 processed, detection: (np.float32(886.5675), np.float32(271.05548))\n",
      "Frame 15 processed, detection: None\n",
      "Frame 16 processed, detection: None\n",
      "Frame 17 processed, detection: (np.float32(855.8163), np.float32(272.81354))\n",
      "Frame 18 processed, detection: None\n",
      "Frame 19 processed, detection: (np.float32(949.66125), np.float32(251.41714))\n",
      "Frame 20 processed, detection: None\n",
      "Frame 21 processed, detection: (np.float32(954.9358), np.float32(262.84103))\n",
      "Frame 22 processed, detection: None\n",
      "Frame 23 processed, detection: (np.float32(1000.1653), np.float32(281.81128))\n",
      "Frame 24 processed, detection: None\n",
      "Frame 25 processed, detection: None\n",
      "Frame 26 processed, detection: (np.float32(939.64496), np.float32(361.43945))\n",
      "Frame 27 processed, detection: (np.float32(939.6916), np.float32(361.76532))\n",
      "Frame 28 processed, detection: (np.float32(997.65186), np.float32(296.28833))\n",
      "Frame 29 processed, detection: (np.float32(938.80994), np.float32(361.38873))\n",
      "Frame 30 processed, detection: (np.float32(938.2448), np.float32(361.8595))\n",
      "\n",
      "Detection Phase Complete.\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(os.path.join(MODEL_PATH, 'cricket_ball_detector_model_v8m.pt'))\n",
    "VIDEO_PATH = r\"test_vids\\1.mp4\"\n",
    "OUTPUT_PATH = r\"output_videos\\1_tracked.mp4\"\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# SETUP PHASE (SCALED INTERACTIVE)\n",
    "# ==========================================\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "ret, first_frame = cap.read()\n",
    "\n",
    "if not ret:\n",
    "    print(\"Error: Could not read video.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Launching Setup...\")\n",
    "\n",
    "# STEP 1: Define the \"Active Play Area\"\n",
    "# Everything OUTSIDE this box will be ignored (set to black).\n",
    "# Good for removing scoreboards, crowd, and sky.\n",
    "roi_active = select_roi_scaled(\"STEP 1: Draw ACTIVE PLAY AREA (Ignore everything else), press space/enter to confirm, C to cancel\", first_frame)\n",
    "\n",
    "# STEP 2: Define the \"Start Zone\"\n",
    "# The ball MUST appear inside this box first to start tracking.\n",
    "# (e.g., Bowler's run-up / Release point)\n",
    "roi_start = select_roi_scaled(\"STEP 2: Draw START ZONE (Where ball appears first), press space/enter to confirm, C to cancel\", first_frame)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"Selected ROIs:\")\n",
    "print(f\"Active Play Area: {roi_active}\")\n",
    "print(f\"Start Zone: {roi_start}\")\n",
    "\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 0) # Reset video\n",
    "\n",
    "# ==========================================\n",
    "# MAIN DETECTION LOOP\n",
    "# ==========================================\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "pre_detections = []\n",
    "frame_id = 0\n",
    "has_locked_on = False\n",
    "\n",
    "# Unpack ROI coordinates\n",
    "ax, ay, aw, ah = roi_active\n",
    "has_active_area = aw > 0 and ah > 0\n",
    "\n",
    "sx, sy, sw, sh = roi_start\n",
    "has_start_zone = sw > 0 and sh > 0\n",
    "\n",
    "frames_since_last_valid = 0\n",
    "\n",
    "print(f\"\\nProcessing Video (Resolution: {frame_width}x{frame_height})...\")\n",
    "\n",
    "IMGSZ = 1920\n",
    "CONF = 0.00035\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # 1. APPLY ACTIVE AREA MASK\n",
    "    # Instead of cropping (which messes up coordinates), we paint the \"Ignore\" areas black.\n",
    "    if has_active_area:\n",
    "        mask = np.zeros_like(frame)\n",
    "        # Draw a white rectangle where the active area is\n",
    "        cv2.rectangle(mask, (ax, ay), (ax+aw, ay+ah), (255, 255, 255), -1)\n",
    "        # Apply mask: Keep active area, make everything else black\n",
    "        frame = cv2.bitwise_and(frame, mask)\n",
    "\n",
    "    \n",
    "    # 2. PREDICT\n",
    "    results = model.predict(source=frame, imgsz=IMGSZ, conf=CONF, device=DEVICE, verbose=False)\n",
    "\n",
    "    best_candidate = None\n",
    "    min_dist = float('inf')\n",
    "\n",
    "    # Get last known detection\n",
    "    last_detection = None\n",
    "    if pre_detections:\n",
    "        for det in reversed(pre_detections):\n",
    "            if det is not None:\n",
    "                last_detection = det\n",
    "                break\n",
    "\n",
    "    # 3. FILTER\n",
    "    closest_detections = []\n",
    "    for result in results:\n",
    "        boxes = result.boxes\n",
    "        for i, box in enumerate(boxes):\n",
    "            # Get coordinates and Confidence\n",
    "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "            confidence = box.conf[0].item() # Extract confidence score\n",
    "            \n",
    "            box_w, box_h = x2 - x1, y2 - y1\n",
    "            box_area = box_w * box_h\n",
    "            aspect_ratio = box_w / box_h if box_h != 0 else 0\n",
    "\n",
    "            # Relaxed filters\n",
    "            MAX_AREA = 500\n",
    "            MIN_AREA = 40\n",
    "\n",
    "            if MIN_AREA < box_area < MAX_AREA and 0.7 < aspect_ratio < 1.6:\n",
    "                center_x = (x1 + x2) / 2\n",
    "                center_y = (y1 + y2) / 2\n",
    "                # Store (x, y, conf) tuple\n",
    "                closest_detections.append((center_x, center_y, confidence))\n",
    "\n",
    "    # 4. SELECT CANDIDATE (Hybrid Confidence/Distance)\n",
    "    if closest_detections:\n",
    "        # Sort candidates by Confidence (Highest first)\n",
    "        closest_detections.sort(key=lambda x: x[2], reverse=True)\n",
    "        \n",
    "        if last_detection:\n",
    "            last_x, last_y = last_detection\n",
    "            found_valid = False\n",
    "            \n",
    "            # --- TIERED DISTANCE LOGIC ---\n",
    "            # 1. Standard Limit: Grows with gap size (150px per frame)\n",
    "            # 2. Hard Limit: Absolute max jump allowed even for high conf (e.g., 1000px)\n",
    "            dynamic_limit = 100 + 5*frames_since_last_valid\n",
    "            hard_limit = 250 \n",
    "            \n",
    "            for det in closest_detections:\n",
    "                center_x, center_y, conf = det\n",
    "                dist = np.sqrt((center_x - last_x) ** 2 + (center_y - last_y) ** 2)\n",
    "                \n",
    "                # LOGIC:\n",
    "                # If Conf is VERY HIGH (>0.6), allow huge jumps (use hard_limit).\n",
    "                # If Conf is LOW, enforce strict physics (use dynamic_limit).\n",
    "                \n",
    "                is_high_confidence = conf > 0.4\n",
    "                \n",
    "                limit_to_use = hard_limit if is_high_confidence else dynamic_limit\n",
    "                \n",
    "                if dist < limit_to_use:\n",
    "                    best_candidate = (center_x, center_y)\n",
    "                    found_valid = True\n",
    "                    # print(f\"Selected Conf {conf:.2f} at dist {dist:.0f} (Limit: {limit_to_use})\")\n",
    "                    break \n",
    "            \n",
    "            if not found_valid:\n",
    "                best_candidate = None\n",
    "        else:\n",
    "            # First frame ever\n",
    "            best_candidate = closest_detections[0][:2]\n",
    "\n",
    "    # # 4. SELECT CANDIDATE\n",
    "    # if closest_detections:\n",
    "    #     if last_detection:\n",
    "    #         last_x, last_y = last_detection\n",
    "    #         for det in closest_detections:\n",
    "    #             center_x, center_y = det\n",
    "    #             dist = np.sqrt((center_x - last_x) ** 2 + (center_y - last_y) ** 2)\n",
    "                \n",
    "    #             if dist < min_dist and dist < 200: \n",
    "    #                 min_dist = dist\n",
    "    #                 best_candidate = (center_x, center_y)\n",
    "    #     else:\n",
    "    #         best_candidate = closest_detections[0]\n",
    "\n",
    "    # 5. STATE MACHINE\n",
    "    if not has_locked_on:\n",
    "        # --- SCANNING MODE ---\n",
    "        if best_candidate:\n",
    "            center_x, center_y = best_candidate\n",
    "            \n",
    "            is_valid_start = False\n",
    "            \n",
    "            if has_start_zone:\n",
    "                # Use User-Defined Box\n",
    "                if (sx < center_x < sx + sw) and (sy < center_y < sy + sh):\n",
    "                    is_valid_start = True\n",
    "            else:\n",
    "                # Default Logic (Broad Center)\n",
    "                if (0.2 * frame_width < center_x < 0.8 * frame_width) and \\\n",
    "                   (0.3 * frame_height < center_y < 0.85 * frame_height):\n",
    "                    is_valid_start = True\n",
    "\n",
    "            if is_valid_start:\n",
    "                pre_detections.append(best_candidate)\n",
    "                has_locked_on = True \n",
    "                print(f\"LOCKED ON at Frame {frame_id}: {best_candidate}\")\n",
    "            else:\n",
    "                pre_detections.append(None)\n",
    "        else:\n",
    "            pre_detections.append(None)\n",
    "    else:\n",
    "        # --- TRACKING MODE ---\n",
    "        pre_detections.append(best_candidate)\n",
    "    \n",
    "    if best_candidate:\n",
    "        frames_since_last_valid = 0\n",
    "    else:\n",
    "        frames_since_last_valid += 1\n",
    "\n",
    "    print(f'Frame {frame_id} processed, detection: {pre_detections[-1]}')\n",
    "    frame_id += 1\n",
    "\n",
    "cap.release()\n",
    "print(\"\\nDetection Phase Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1195,
   "id": "d9f14158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "(np.float32(898.2599), np.float32(168.03787))\n",
      "(np.float32(906.7438), np.float32(166.49026))\n",
      "(np.float32(913.0237), np.float32(166.47955))\n",
      "(np.float32(918.3371), np.float32(172.3624))\n",
      "(np.float32(897.0719), np.float32(234.38097))\n",
      "(np.float32(885.33136), np.float32(228.20984))\n",
      "(np.float32(931.7075), np.float32(199.24944))\n",
      "(np.float32(902.5604), np.float32(273.55188))\n",
      "(np.float32(888.6969), np.float32(272.826))\n",
      "(np.float32(886.5675), np.float32(271.05548))\n",
      "None\n",
      "None\n",
      "(np.float32(855.8163), np.float32(272.81354))\n",
      "None\n",
      "(np.float32(949.66125), np.float32(251.41714))\n",
      "None\n",
      "(np.float32(954.9358), np.float32(262.84103))\n",
      "None\n",
      "(np.float32(1000.1653), np.float32(281.81128))\n",
      "None\n",
      "None\n",
      "(np.float32(939.64496), np.float32(361.43945))\n",
      "(np.float32(939.6916), np.float32(361.76532))\n",
      "(np.float32(997.65186), np.float32(296.28833))\n",
      "(np.float32(938.80994), np.float32(361.38873))\n",
      "(np.float32(938.2448), np.float32(361.8595))\n"
     ]
    }
   ],
   "source": [
    "for det in pre_detections:\n",
    "    print(det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1196,
   "id": "46e4114a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ball track confirmed at frame 5. Cleaning prior noise.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# PHASE 2: STREAK FILTER (Remove Initial Noise)\n",
    "# ==========================================\n",
    "# This removes short blips of noise before the actual continuous ball path starts.\n",
    "MIN_STREAK = 2 \n",
    "cleaned_detections = pre_detections.copy()\n",
    "streak_count = 0\n",
    "start_index = -1\n",
    "found_start = False\n",
    "\n",
    "for i, det in enumerate(cleaned_detections):\n",
    "    if det is not None:\n",
    "        if streak_count == 0:\n",
    "            start_index = i\n",
    "        streak_count += 1\n",
    "        if streak_count >= MIN_STREAK:\n",
    "            found_start = True\n",
    "            break \n",
    "    else:\n",
    "        streak_count = 0\n",
    "\n",
    "if found_start:\n",
    "    print(f\"Ball track confirmed at frame {start_index}. Cleaning prior noise.\")\n",
    "    # Set everything before the valid start to None\n",
    "    for k in range(start_index):\n",
    "        cleaned_detections[k] = None\n",
    "else:\n",
    "    print(\"Warning: No consistent ball track found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1197,
   "id": "575b0f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==========================================\n",
    "# # PHASE 3: INTERPOLATION (Gap-Aware & End-Clamped)\n",
    "# # ==========================================\n",
    "# print(\"Running Interpolation...\")\n",
    "\n",
    "# post_processed_detections = []\n",
    "\n",
    "# # 1. Convert to Numpy\n",
    "# pre_detections_np = np.array([det if det is not None else (np.nan, np.nan) for det in cleaned_detections])\n",
    "\n",
    "# # 2. OUTLIER REMOVAL\n",
    "# valid_indices = np.where(~np.isnan(pre_detections_np[:, 0]))[0]\n",
    "# if len(valid_indices) > 0:\n",
    "#     last_idx = valid_indices[0]\n",
    "#     for idx in valid_indices[1:]:\n",
    "#         curr_pt = pre_detections_np[idx]\n",
    "#         last_pt = pre_detections_np[last_idx]\n",
    "#         dist = np.linalg.norm(curr_pt - last_pt)\n",
    "#         gap_size = idx - last_idx\n",
    "        \n",
    "#         # Dynamic threshold: Allow 100px movement per frame of gap\n",
    "#         if dist > (100 * gap_size):\n",
    "#             pre_detections_np[idx] = (np.nan, np.nan) \n",
    "#         else:\n",
    "#             last_idx = idx\n",
    "\n",
    "# # 3. INTERPOLATION\n",
    "# valid_mask = ~np.isnan(pre_detections_np[:, 0])\n",
    "# valid_indices = np.where(valid_mask)[0]\n",
    "\n",
    "# if len(valid_indices) > 1:\n",
    "#     # Identify Start AND End boundaries\n",
    "#     first_valid_idx = valid_indices[0]\n",
    "#     last_valid_idx = valid_indices[-1]  # <--- CRITICAL FIX: Find the last real ball\n",
    "\n",
    "#     x = np.arange(len(pre_detections_np))\n",
    "    \n",
    "#     # Quadratic vs Linear decision\n",
    "#     kind_type = 'quadratic' if len(valid_indices) > 3 else 'linear'\n",
    "    \n",
    "#     try:\n",
    "#         f_x = interpolate.interp1d(x[valid_mask], pre_detections_np[:, 0][valid_mask], kind=kind_type, fill_value=\"extrapolate\")\n",
    "#         f_y = interpolate.interp1d(x[valid_mask], pre_detections_np[:, 1][valid_mask], kind=kind_type, fill_value=\"extrapolate\")\n",
    "#     except:\n",
    "#         f_x = interpolate.interp1d(x[valid_mask], pre_detections_np[:, 0][valid_mask], kind='linear', fill_value=\"extrapolate\")\n",
    "#         f_y = interpolate.interp1d(x[valid_mask], pre_detections_np[:, 1][valid_mask], kind='linear', fill_value=\"extrapolate\")\n",
    "\n",
    "#     for i in range(len(pre_detections_np)):\n",
    "#         # CLAMPING LOGIC:\n",
    "#         # If we are BEFORE the first ball OR AFTER the last ball, return (0,0)\n",
    "#         # This prevents the line from shooting off into infinity at the end.\n",
    "#         if i < first_valid_idx or i > last_valid_idx:\n",
    "#             post_processed_detections.append((0.0, 0.0))\n",
    "#         else:\n",
    "#             post_processed_detections.append((float(f_x(i)), float(f_y(i))))\n",
    "\n",
    "#     # 4. GAP CUTTING\n",
    "#     MAX_GAP = 10 \n",
    "    \n",
    "#     for k in range(len(valid_indices) - 1):\n",
    "#         idx_current = valid_indices[k]\n",
    "#         idx_next = valid_indices[k+1]\n",
    "        \n",
    "#         gap_len = idx_next - idx_current\n",
    "        \n",
    "#         if gap_len > MAX_GAP:\n",
    "#             print(f\"Cutting large gap of {gap_len} frames between {idx_current} and {idx_next}\")\n",
    "#             for g in range(idx_current + 1, idx_next):\n",
    "#                 post_processed_detections[g] = (0.0, 0.0)\n",
    "\n",
    "#     print(f\"Interpolation finished using {kind_type} fit.\")\n",
    "\n",
    "# else:\n",
    "#     print(\"Not enough data points to interpolate.\")\n",
    "#     post_processed_detections = [(0.0, 0.0)] * len(pre_detections_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1198,
   "id": "fac0631c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Physics-Aware Interpolation...\n",
      "Interpolation Complete. Bounce detected at Frame 30.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# PHASE 3: PIECEWISE INTERPOLATION (Physics-Aware)\n",
    "# ==========================================\n",
    "print(\"Running Physics-Aware Interpolation...\")\n",
    "\n",
    "post_processed_detections = []\n",
    "\n",
    "# 1. Convert to Numpy\n",
    "pre_detections_np = np.array([det if det is not None else (np.nan, np.nan) for det in cleaned_detections])\n",
    "\n",
    "# 2. OUTLIER REMOVAL (Jump Killer) - KEEPING YOUR LOGIC\n",
    "valid_indices = np.where(~np.isnan(pre_detections_np[:, 0]))[0]\n",
    "if len(valid_indices) > 0:\n",
    "    last_idx = valid_indices[0]\n",
    "    for idx in valid_indices[1:]:\n",
    "        curr_pt = pre_detections_np[idx]\n",
    "        last_pt = pre_detections_np[last_idx]\n",
    "        dist = np.linalg.norm(curr_pt - last_pt)\n",
    "        gap_size = idx - last_idx\n",
    "        \n",
    "        # Dynamic threshold: Allow 100px movement per frame of gap\n",
    "        if dist > (100 * gap_size):\n",
    "            pre_detections_np[idx] = (np.nan, np.nan) \n",
    "        else:\n",
    "            last_idx = idx\n",
    "\n",
    "# 3. SPLIT INTERPOLATION (Bounce Handling)\n",
    "valid_mask = ~np.isnan(pre_detections_np[:, 0])\n",
    "valid_indices = np.where(valid_mask)[0]\n",
    "\n",
    "if len(valid_indices) > 2:\n",
    "    first_valid_idx = valid_indices[0]\n",
    "    last_valid_idx = valid_indices[-1]\n",
    "    \n",
    "    # --- A. DETECT BOUNCE (Find Max Y) ---\n",
    "    # In OpenCV, Y increases downwards. The \"lowest\" point on screen is the Max Y.\n",
    "    # We look at the valid y-values to find the peak.\n",
    "    \n",
    "    valid_y_values = pre_detections_np[valid_indices, 1]\n",
    "    \n",
    "    # Find the index of the bounce in the VALID array\n",
    "    argmax_y = np.argmax(valid_y_values) \n",
    "    bounce_idx_global = valid_indices[argmax_y] # The actual frame number of the bounce\n",
    "    \n",
    "    # Logic: Only split if the bounce is somewhat in the middle (not start/end)\n",
    "    # and \"sharp\" enough (we assume Max Y is the bounce).\n",
    "    \n",
    "    # Split data into two segments:\n",
    "    # Segment 1: Start -> Bounce (Delivery)\n",
    "    # Segment 2: Bounce -> End (Rebound)\n",
    "    \n",
    "    # Indices for Segment 1\n",
    "    mask_s1 = (valid_indices <= bounce_idx_global)\n",
    "    indices_s1 = valid_indices[mask_s1]\n",
    "    \n",
    "    # Indices for Segment 2\n",
    "    mask_s2 = (valid_indices >= bounce_idx_global)\n",
    "    indices_s2 = valid_indices[mask_s2]\n",
    "    \n",
    "    # Define a helper to interpolate a segment\n",
    "    # Define a helper to interpolate a segment\n",
    "    def interpolate_segment(indices, full_len):\n",
    "        if len(indices) < 2: return None\n",
    "        \n",
    "        x_local = indices\n",
    "        y_x_local = pre_detections_np[indices, 0]\n",
    "        y_y_local = pre_detections_np[indices, 1]\n",
    "        \n",
    "        # --- THE FIX: SEPARATE PHYSICS FOR X AND Y ---\n",
    "        \n",
    "        # 1. X-AXIS (Horizontal): Always use LINEAR.\n",
    "        # Physics: Air resistance is negligible over 20 meters. \n",
    "        # The ball moves sideways at constant velocity.\n",
    "        # This prevents \"overshoot\" or \"snaking\" left/right.\n",
    "        try:\n",
    "            fx = interpolate.interp1d(x_local, y_x_local, kind='linear', fill_value=\"extrapolate\")\n",
    "        except:\n",
    "            return None # Should not happen if len >= 2\n",
    "\n",
    "        # 2. Y-AXIS (Vertical): Use QUADRATIC (Gravity).\n",
    "        # Physics: Gravity causes acceleration (parabola).\n",
    "        # Exception: If we have too few points (<4), quadratic math is unstable. Fallback to linear.\n",
    "        try:\n",
    "            kind_y = 'quadratic' if len(indices) > 3 else 'linear'\n",
    "            fy = interpolate.interp1d(x_local, y_y_local, kind=kind_y, fill_value=\"extrapolate\")\n",
    "        except:\n",
    "            # Fallback if quadratic fails\n",
    "            fy = interpolate.interp1d(x_local, y_y_local, kind='linear', fill_value=\"extrapolate\")\n",
    "            \n",
    "        return fx, fy\n",
    "\n",
    "    # Interpolate both halves\n",
    "    funcs_s1 = interpolate_segment(indices_s1, len(pre_detections_np))\n",
    "    funcs_s2 = interpolate_segment(indices_s2, len(pre_detections_np))\n",
    "    \n",
    "    # --- B. FILL DATA ---\n",
    "    for i in range(len(pre_detections_np)):\n",
    "        # CLAMP: Before start or after end -> (0,0)\n",
    "        if i < first_valid_idx or i > last_valid_idx:\n",
    "            post_processed_detections.append((0.0, 0.0))\n",
    "            continue\n",
    "            \n",
    "        # Decide which curve to use\n",
    "        # If we are before the bounce, use Curve 1. If after, use Curve 2.\n",
    "        if i <= bounce_idx_global and funcs_s1:\n",
    "            fx, fy = funcs_s1\n",
    "            post_processed_detections.append((float(fx(i)), float(fy(i))))\n",
    "        elif i > bounce_idx_global and funcs_s2:\n",
    "            fx, fy = funcs_s2\n",
    "            post_processed_detections.append((float(fx(i)), float(fy(i))))\n",
    "        else:\n",
    "            # Fallback if split failed (e.g. no bounce found, straight line)\n",
    "            # Use a simple linear fit across the whole thing\n",
    "            if i == first_valid_idx: # Only print once\n",
    "                print(\"Bounce split failed, falling back to global linear fit.\")\n",
    "                \n",
    "            x_all = valid_indices\n",
    "            fx = interpolate.interp1d(x_all, pre_detections_np[x_all, 0], kind='linear', fill_value=\"extrapolate\")\n",
    "            fy = interpolate.interp1d(x_all, pre_detections_np[x_all, 1], kind='linear', fill_value=\"extrapolate\")\n",
    "            post_processed_detections.append((float(fx(i)), float(fy(i))))\n",
    "\n",
    "    # 4. GAP CUTTING (Occlusion Handling)\n",
    "    MAX_GAP = 15\n",
    "    for k in range(len(valid_indices) - 1):\n",
    "        idx_current = valid_indices[k]\n",
    "        idx_next = valid_indices[k+1]\n",
    "        gap_len = idx_next - idx_current\n",
    "        \n",
    "        if gap_len > MAX_GAP:\n",
    "            print(f\"Cutting large gap of {gap_len} frames.\")\n",
    "            for g in range(idx_current + 1, idx_next):\n",
    "                post_processed_detections[g] = (0.0, 0.0)\n",
    "\n",
    "    print(f\"Interpolation Complete. Bounce detected at Frame {bounce_idx_global}.\")\n",
    "\n",
    "else:\n",
    "    print(\"Not enough data points to interpolate.\")\n",
    "    post_processed_detections = [(0.0, 0.0)] * len(pre_detections_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1199,
   "id": "99dbddf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 0.0)\n",
      "(0.0, 0.0)\n",
      "(0.0, 0.0)\n",
      "(0.0, 0.0)\n",
      "(0.0, 0.0)\n",
      "(898.2598876953125, 168.03787231445312)\n",
      "(906.7437744140625, 166.49026489257812)\n",
      "(913.023681640625, 166.47955322265625)\n",
      "(918.3370971679688, 172.36239624023435)\n",
      "(897.0718994140625, 234.3809661865234)\n",
      "(885.3313598632812, 228.20983886718747)\n",
      "(931.70751953125, 199.2494354248047)\n",
      "(902.5604248046875, 273.5518798828125)\n",
      "(888.6968994140625, 272.82598876953125)\n",
      "(886.5675048828125, 271.05548095703125)\n",
      "(876.3170979817709, 273.47433076423766)\n",
      "(866.0666910807291, 276.55551156494545)\n",
      "(855.8162841796875, 272.81353759765625)\n",
      "(902.73876953125, 261.0008279021204)\n",
      "(949.6612548828125, 251.41714477539062)\n",
      "(952.2985229492188, 254.3622505145195)\n",
      "(954.935791015625, 262.8410339355468)\n",
      "(977.550537109375, 269.8583838545126)\n",
      "(1000.165283203125, 281.811279296875)\n",
      "(979.9918416341146, 305.0966992880921)\n",
      "(959.8184000651041, 337.02290385126514)\n",
      "(939.6449584960938, 361.439453125)\n",
      "(939.6915893554688, 361.76531982421875)\n",
      "(997.65185546875, 296.288330078125)\n",
      "(938.8099365234375, 361.38873291015625)\n",
      "(938.2448120117188, 361.8594970703125)\n"
     ]
    }
   ],
   "source": [
    "for det in post_processed_detections:\n",
    "    print(det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1200,
   "id": "265db249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 4: Saving Video...\n",
      "Done! Video saved to: output_videos\\1_tracked.mp4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# PHASE 4: VISUALIZATION (Broadcast Style)\n",
    "# ==========================================\n",
    "print(\"Phase 4: Saving Video...\")\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "# Ensure output path exists\n",
    "output_dir = os.path.dirname(OUTPUT_PATH)\n",
    "if output_dir and not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Define codec and create VideoWriter\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v') # or 'avc1' or 'XVID' depending on your OS\n",
    "out = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "frame_id = 0\n",
    "trace_points = [] # Permanent history of valid points\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Safety check in case video is longer than processed frames\n",
    "    if frame_id >= len(post_processed_detections):\n",
    "        break\n",
    "\n",
    "    detection = post_processed_detections[frame_id]\n",
    "    cx, cy = map(int, detection)\n",
    "\n",
    "    # Check if point is valid (we set gaps to 0,0 during interpolation)\n",
    "    is_valid_point = (cx > 5 and cy > 5) # Small buffer to avoid corners\n",
    "\n",
    "    if is_valid_point:\n",
    "        trace_points.append((cx, cy))\n",
    "\n",
    "    # --- 1. Draw Translucent White Tail ---\n",
    "    # Only draw if we have at least 2 points to make a line\n",
    "    if len(trace_points) > 1:\n",
    "        # Create a copy of the frame for the overlay layer\n",
    "        overlay = frame.copy()\n",
    "        \n",
    "        # Convert list of points to the numpy format required by polylines\n",
    "        pts_np = np.array(trace_points, np.int32).reshape((-1, 1, 2))\n",
    "        \n",
    "        # Draw solid white line on the overlay\n",
    "        # color=(255,255,255) is White in BGR\n",
    "        cv2.polylines(overlay, [pts_np], isClosed=False, color=(255, 255, 255), thickness=5, lineType=cv2.LINE_AA)\n",
    "        \n",
    "        # Blend the overlay with the original frame\n",
    "        # alpha = opacity of the tail (0.0 to 1.0). 0.4 = 40% visible.\n",
    "        alpha = 0.4\n",
    "        frame = cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0)\n",
    "\n",
    "    # --- 2. Draw Solid Red Current Ball ---\n",
    "    if is_valid_point:\n",
    "        # color=(0, 0, 255) is Red in BGR\n",
    "        # -1 thickness means filled circle\n",
    "        cv2.circle(frame, (cx, cy), 3, (0, 0, 255), -1)\n",
    "\n",
    "    out.write(frame)\n",
    "    frame_id += 1\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "print(f\"Done! Video saved to: {OUTPUT_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
